\relax 
\newlabel{FirstPage}{{}{1}{}{}{}}
\@writefile{toc}{\contentsline {title}{Energy Clustering}{1}{}}
\@writefile{toc}{\contentsline {abstract}{Abstract}{1}{}}
\citation{Szkely2013}
\citation{RizzoVariance}
\citation{RizzoClustering}
\citation{Kgroups}
\citation{Szkely2013}
\citation{Lyons}
\citation{Sejdinovic2013}
\citation{Lloyd,MacQueen,Forgy}
\citation{Lloyd}
\citation{Smola,Girolami}
\citation{Mercer}
\citation{Girolami}
\citation{Dhillon2,Dhillon}
\citation{Lloyd}
\citation{Filippone}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{2}{}}
\citation{Kgroups}
\citation{Dhillon,Dhillon2}
\citation{Hartigan}
\citation{Kgroups}
\citation{Telgarsky,Slonin}
\citation{Szkely2013}
\citation{Sejdinovic2013}
\citation{Szkely2013}
\@writefile{toc}{\contentsline {section}{\numberline {II}Background on Energy Statistics and RKHS}{4}{}}
\newlabel{sec:background}{{II}{4}{}{}{}}
\newlabel{eq:energy}{{1}{4}{}{}{}}
\newlabel{eq:energy2}{{2}{4}{}{}{}}
\citation{Sejdinovic2013}
\citation{Aronszajn}
\newlabel{eq:negative_type}{{II}{5}{}{}{}}
\MT@newlabel{eq:energy}
\newlabel{eq:energy3}{{3}{5}{}{}{}}
\MT@newlabel{eq:energy3}
\citation{Gretton2012}
\citation{Berg1984}
\citation{Sejdinovic2013}
\citation{Sejdinovic2013}
\newlabel{eq:mmd}{{4}{6}{}{}{}}
\newlabel{eq:mmd2}{{5}{6}{}{}{}}
\MT@newlabel{eq:mmd}
\MT@newlabel{eq:mmd2}
\newlabel{eq:inner_data}{{II}{6}{}{}{}}
\newlabel{eq:kernel_semimetric}{{6}{6}{}{}{}}
\newlabel{eq:gen_kernel}{{7}{6}{}{}{}}
\citation{Szkely2013}
\citation{Szkely2013}
\MT@newlabel{eq:energy3}
\MT@newlabel{eq:gen_kernel}
\MT@newlabel{eq:energy3}
\MT@newlabel{eq:mmd2}
\newlabel{eq:Erho}{{II}{7}{}{}{}}
\MT@newlabel{eq:mmd}
\MT@newlabel{eq:energy3}
\newlabel{eq:g_def}{{8}{7}{}{}{}}
\newlabel{eq:within}{{9}{7}{}{}{}}
\newlabel{eq:between}{{10}{7}{}{}{}}
\citation{Kgroups}
\@writefile{toc}{\contentsline {section}{\numberline {III}Clustering Based on Energy Statistics}{8}{}}
\newlabel{sec:clustering_theory}{{III}{8}{}{}{}}
\newlabel{th:minimize}{{1}{8}{}{}{}}
\MT@newlabel{eq:between}
\newlabel{eq:minimize}{{11}{8}{}{}{}}
\MT@newlabel{eq:within}
\MT@newlabel{eq:within}
\MT@newlabel{eq:between}
\MT@newlabel{eq:minimize}
\MT@newlabel{eq:minimize}
\newlabel{eq:kernel_matrix}{{12}{9}{}{}{}}
\newlabel{eq:label_matrix}{{13}{9}{}{}{}}
\MT@newlabel{eq:minimize}
\newlabel{th:qcqp2}{{2}{9}{}{}{}}
\MT@newlabel{eq:minimize}
\newlabel{eq:qcqp2}{{14}{9}{}{}{}}
\MT@newlabel{eq:kernel_matrix}
\MT@newlabel{eq:gen_kernel}
\MT@newlabel{eq:g_def}
\MT@newlabel{eq:within}
\newlabel{eq:W2}{{15}{9}{}{}{}}
\MT@newlabel{eq:W2}
\newlabel{eq:max_prob}{{16}{9}{}{}{}}
\citation{Malik,NgJordan}
\MT@newlabel{eq:kernel_matrix}
\MT@newlabel{eq:label_matrix}
\MT@newlabel{eq:max_prob}
\MT@newlabel{eq:label_matrix}
\newlabel{eq:qcqp}{{17}{10}{}{}{}}
\MT@newlabel{eq:qcqp2}
\MT@newlabel{eq:qcqp}
\MT@newlabel{eq:qcqp}
\MT@newlabel{eq:qcqp2}
\MT@newlabel{eq:qcqp2}
\newlabel{eq:relaxed}{{III}{10}{}{}{}}
\citation{Lloyd}
\citation{Dhillon2,Dhillon}
\MT@newlabel{eq:qcqp2}
\MT@newlabel{eq:energy2}
\MT@newlabel{eq:kernel_semimetric}
\MT@newlabel{eq:qcqp2}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Relation to Kernel $\bm  {k}$-Means}{11}{}}
\MT@newlabel{eq:kernel_matrix}
\newlabel{eq:kernel_kmeans}{{18}{11}{}{}{}}
\MT@newlabel{eq:kernel_kmeans}
\MT@newlabel{eq:qcqp2}
\MT@newlabel{eq:minimize}
\MT@newlabel{eq:kernel_kmeans}
\newlabel{th:kernel_kmeans}{{3}{11}{}{}{}}
\MT@newlabel{eq:minimize}
\MT@newlabel{eq:kernel_kmeans}
\MT@newlabel{eq:qcqp2}
\MT@newlabel{eq:kernel_kmeans}
\MT@newlabel{eq:kernel_kmeans}
\citation{Dhillon2,Dhillon}
\newlabel{eq:J}{{19}{12}{}{}{}}
\MT@newlabel{eq:kernel_kmeans}
\MT@newlabel{eq:max_prob}
\MT@newlabel{eq:within}
\MT@newlabel{eq:kernel_kmeans}
\MT@newlabel{eq:qcqp2}
\MT@newlabel{eq:kernel_kmeans}
\MT@newlabel{eq:qcqp2}
\MT@newlabel{eq:qcqp2}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Clustering Based on Weighted Energy Statistics}{12}{}}
\newlabel{sec:weighted}{{IV}{12}{}{}{}}
\newlabel{eq:g_def2}{{20}{12}{}{}{}}
\MT@newlabel{eq:within}
\MT@newlabel{eq:between}
\newlabel{eq:minimize2}{{21}{13}{}{}{}}
\MT@newlabel{eq:g_def2}
\newlabel{eq:weighted_matrices}{{22}{13}{}{}{}}
\newlabel{th:qcqp3}{{4}{13}{}{}{}}
\MT@newlabel{eq:minimize2}
\newlabel{eq:qcqp3}{{23}{13}{}{}{}}
\MT@newlabel{eq:kernel_matrix}
\MT@newlabel{eq:gen_kernel}
\MT@newlabel{eq:minimize2}
\citation{Dhillon2,Dhillon}
\citation{Kernighan,Malik,Chan,Yu}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Connection with Graph Partitioning}{14}{}}
\newlabel{eq:assoc}{{24}{14}{}{}{}}
\newlabel{eq:cut}{{25}{14}{}{}{}}
\MT@newlabel{eq:within}
\MT@newlabel{eq:between}
\MT@newlabel{eq:assoc}
\MT@newlabel{eq:cut}
\MT@newlabel{eq:assoc}
\MT@newlabel{eq:label_matrix}
\MT@newlabel{eq:weighted_matrices}
\MT@newlabel{eq:qcqp3}
\MT@newlabel{eq:gen_kernel}
\newlabel{eq:metric_graphs}{{26}{15}{}{}{}}
\MT@newlabel{eq:metric_graphs}
\@writefile{toc}{\contentsline {section}{\numberline {V}Two-Class Problem in One Dimension}{15}{}}
\newlabel{sec:twoclass}{{V}{15}{}{}{}}
\MT@newlabel{eq:qcqp2}
\MT@newlabel{eq:g_def}
\newlabel{eq:g_ind}{{V}{15}{}{}{}}
\newlabel{algo1d}{{1}{16}{}{}{}}
\MT@newlabel{eq:minimize}
\MT@newlabel{eq:w1d}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces   $\mathcal  {E}^{1D}$-clustering algorithm to find local solutions to the optimization problem \MT_extended_eqref:n  {eq:minimize} for a two-class problem in one dimension. \hspace  {\fill } }}{16}{}}
\newlabel{eq:g1d}{{V}{16}{}{}{}}
\newlabel{eq:w1d}{{27}{16}{}{}{}}
\MT@newlabel{eq:w1d}
\MT@newlabel{eq:w1d}
\citation{scikit-learn}
\citation{Vassilvitskii}
\citation{Dhillon2,Dhillon}
\newlabel{eq:accuracy}{{28}{17}{}{}{}}
\MT@newlabel{eq:accuracy}
\newlabel{eq:two_normal}{{29}{17}{}{}{}}
\newlabel{eq:two_lognormal}{{30}{17}{}{}{}}
\MT@newlabel{eq:two_lognormal}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces   $\mathcal  {E}^{1D}$-clustering versus $k$-means and GMM. (a,b) We plot the mean accuracy \MT_extended_eqref:n  {eq:accuracy} over $100$ Monte Carlo trials, versus the number of sampled points. Error bars are standard error. The dashed line indicates Bayes accuracy ($\approx 0.88$ in both cases). (a) Clustering results for data normally distributed as in \MT_extended_eqref:n  {eq:two_normal}. (b) Data lognormally distributed as in \MT_extended_eqref:n  {eq:two_lognormal}. (c) Density estimation of each component in the mixture \MT_extended_eqref:n  {eq:two_normal} after clustering $1000$ sampled points using the three algorithms, compared to the ground truth. (d) The same but for lognormal data \MT_extended_eqref:n  {eq:two_lognormal}. }}{18}{}}
\newlabel{fig:1d}{{1}{18}{}{}{}}
\MT@newlabel{eq:accuracy}
\MT@newlabel{eq:two_normal}
\MT@newlabel{eq:two_lognormal}
\MT@newlabel{eq:two_normal}
\MT@newlabel{eq:two_lognormal}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Iterative Algorithms for Energy Clustering}{18}{}}
\newlabel{sec:algo}{{VI}{18}{}{}{}}
\MT@newlabel{eq:qcqp2}
\MT@newlabel{eq:max_prob}
\newlabel{eq:maxQ}{{31}{18}{}{}{}}
\citation{Lloyd}
\citation{Dhillon2,Dhillon}
\newlabel{eq:costxij}{{VI}{19}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Lloyd's Method for Energy Clustering}{19}{}}
\MT@newlabel{eq:J}
\newlabel{eq:Jell}{{32}{19}{}{}{}}
\MT@newlabel{eq:Jell}
\citation{Hartigan}
\newlabel{kmeans_algo}{{2}{20}{}{}{}}
\MT@newlabel{eq:qcqp2}
\MT@newlabel{eq:maxQ}
\MT@newlabel{eq:Jell}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces  $\mathcal  {E}^{L}$-clustering is Lloyd's method for energy clustering, which is precisely\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {} \\ kernel $k$-means algorithm, with the kernel induced by energy statistics. This procedure\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {} \\ finds local solutions to the optimization problem \MT_extended_eqref:n  {eq:qcqp2}.\hspace  {\fill } }}{20}{}}
\MT@newlabel{eq:Jell}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Hartigan's Method for Energy Clustering}{20}{}}
\MT@newlabel{eq:maxQ}
\MT@newlabel{eq:qcqp2}
\MT@newlabel{eq:maxQ}
\newlabel{eq:changeQ}{{33}{21}{}{}{}}
\MT@newlabel{eq:maxQ}
\citation{Telgarsky}
\newlabel{algo}{{3}{22}{}{}{}}
\MT@newlabel{eq:qcqp2}
\MT@newlabel{eq:maxQ}
\MT@newlabel{eq:changeQ}
\newlabel{stepmove}{{VI}{22}{}{}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces  $\mathcal  {E}^H$-clustering is Hartigan's method for energy clustering. This algorithm\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {} \\ finds local solutions to the optimization problem \MT_extended_eqref:n  {eq:qcqp2}. The steps $6$ and $10$ are different\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {}\nobreakspace  {} \\ than $\mathcal  {E}^L$-clustering described in Algorithm\nobreakspace  {}2{}{}{}\hbox {}. \hspace  {\fill } }}{22}{}}
\newlabel{noempty}{{1}{22}{}{}{}}
\newlabel{diffmean}{{2}{22}{}{}{}}
\citation{Telgarsky}
\citation{Telgarsky}
\citation{Telgarsky}
\citation{Slonin}
\citation{Slonin}
\citation{scikit-learn}
\citation{Vassilvitskii}
\citation{Malik}
\citation{Telgarsky}
\@writefile{toc}{\contentsline {section}{\numberline {VII}Numerical Experiments}{24}{}}
\newlabel{sec:numerics}{{VII}{24}{}{}{}}
\newlabel{eq:rho_alpha}{{34}{24}{}{}{}}
\newlabel{eq:rho_tilde}{{35}{24}{}{}{}}
\newlabel{eq:rho_hat}{{36}{24}{}{}{}}
\MT@newlabel{eq:kernel_semimetric}
\MT@newlabel{eq:energy}
\MT@newlabel{eq:accuracy}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces   Pair plots for the first $5$ dimensions. (a) Data normally distributed as in \MT_extended_eqref:n  {eq:gauss1}. (b) Data normally distributed as in \MT_extended_eqref:n  {eq:gauss2}. We sample $200$ points for both cases. We can see that there is a considerable overlap between the clusters. }}{25}{}}
\newlabel{fig:pairsplot}{{2}{25}{}{}{}}
\MT@newlabel{eq:gauss1}
\MT@newlabel{eq:gauss2}
\newlabel{eq:gauss1}{{37}{25}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces   Comparison of $\mathcal  {E}^H$-clustering, $\mathcal  {E}^L$-clustering (kernel $k$-means), spectral clustering, $k$-means and GMM in high dimensional Gaussian settings. We plot the mean accuracy versus the number of dimensions, with error bars indicating standard error from $100$ Monte Carlo runs. (a) Data normally distributed as in \MT_extended_eqref:n  {eq:gauss1}, with Bayes accuracy $\approx 0.86$, over the range $D \in [10,200]$. (b) Data normally distributed as in \MT_extended_eqref:n  {eq:gauss2}, with Bayes accuracy $\approx 0.95$, over the range $D \in [10, 700]$. }}{26}{}}
\newlabel{fig:gauss}{{3}{26}{}{}{}}
\MT@newlabel{eq:gauss1}
\MT@newlabel{eq:gauss2}
\newlabel{eq:gauss2}{{38}{26}{}{}{}}
\MT@newlabel{eq:gauss2}
\newlabel{eq:20gauss}{{39}{27}{}{}{}}
\MT@newlabel{eq:rho_alpha}
\MT@newlabel{eq:rho_tilde}
\newlabel{eq:20loggauss}{{40}{27}{}{}{}}
\MT@newlabel{eq:rho_alpha}
\MT@newlabel{eq:rho_tilde}
\MT@newlabel{eq:rho_hat}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces   $\mathcal  {E}^H$-clustering with kernels \MT_extended_eqref:n  {eq:rho_alpha} and \MT_extended_eqref:n  {eq:rho_tilde} versus $k$-means and GMM. In both settings Bayes accuracy is $\approx 0.9$. We show average accuracy (error bars are standard error) versus number of points for $100$ Monte Carlo trials. (a,b) Gaussian mixture \MT_extended_eqref:n  {eq:20gauss}. (c,d) Lognormal mixture \MT_extended_eqref:n  {eq:20loggauss}. The plots in (c) and (d) consider the difference in accuracy between $\mathcal  {E}^H$ versus $\mathcal  {E}^L$ (kernel $k$-means) and spectral clustering, with the kernel $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle K$}\mathaccent "0365{K}_1$. }}{28}{}}
\newlabel{fig:consist}{{4}{28}{}{}{}}
\MT@newlabel{eq:rho_alpha}
\MT@newlabel{eq:rho_tilde}
\MT@newlabel{eq:20gauss}
\MT@newlabel{eq:20loggauss}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  (a) Parallel cigars. (b) Two concentric circles with noise. (c) Three concentric circles with noise. (d) MNIST handwritten digits. Clustering results are in Table\nobreakspace  {}I{}{}{}\hbox {} and Table\nobreakspace  {}II{}{}{}\hbox {}. }}{28}{}}
\newlabel{fig:other}{{5}{28}{}{}{}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces  Clustering data from Fig.\nobreakspace  {}5{}{}{}\hbox {}a--c. }}{29}{}}
\newlabel{table:other}{{I}{29}{}{}{}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces  Clustering MNIST data from Fig.\nobreakspace  {}5{}{}{}\hbox {}d. }}{29}{}}
\newlabel{table:mnist}{{II}{29}{}{}{}}
\citation{Sapiro}
\newlabel{eq:sigma}{{VII}{30}{}{}{}}
\MT@newlabel{eq:rho_tilde}
\MT@newlabel{eq:rho_hat}
\@writefile{toc}{\contentsline {section}{\numberline {VIII}Synapse Data}{30}{}}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces  Features of synapse dataset. }}{30}{}}
\newlabel{table:synapse}{{III}{30}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {IX}Discussion}{30}{}}
\newlabel{sec:conclusion}{{IX}{30}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces   Determining the number of clusters. (a) Largest eigenvalues of kernel matrix. (b) Elbow method. }}{31}{}}
\newlabel{fig:num_clusters}{{6}{31}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces   Determining the number of clusters. (a) Largest eigenvalues of kernel matrix. (b) Elbow method. }}{31}{}}
\newlabel{fig:num_clusters}{{7}{31}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces   Comparison of energy clustering algorithms to $k$-means and GMM on unbalanced clusters. The data is normally distributed as \MT_extended_eqref:n  {eq:gauss3}, where we vary $m \in [0, 240]$, and in each case we do $100$ Monte Carlo runs showing the average accuracy with standard error. }}{32}{}}
\newlabel{fig:unbalanced}{{8}{32}{}{}{}}
\MT@newlabel{eq:gauss3}
\newlabel{eq:gauss3}{{41}{32}{}{}{}}
\bibdata{energy_clusteringNotes,biblio.bib}
\bibcite{Szkely2013}{{1}{}{{}}{{}}}
\bibcite{RizzoVariance}{{2}{}{{}}{{}}}
\bibcite{RizzoClustering}{{3}{}{{}}{{}}}
\bibcite{Kgroups}{{4}{}{{}}{{}}}
\bibcite{Lyons}{{5}{}{{}}{{}}}
\bibcite{Sejdinovic2013}{{6}{}{{}}{{}}}
\bibcite{Lloyd}{{7}{}{{}}{{}}}
\bibcite{MacQueen}{{8}{}{{}}{{}}}
\bibcite{Forgy}{{9}{}{{}}{{}}}
\bibcite{Smola}{{10}{}{{}}{{}}}
\bibcite{Girolami}{{11}{}{{}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {}Acknowledgments}{33}{}}
\@writefile{toc}{\contentsline {section}{\numberline {}References}{33}{}}
\bibcite{Mercer}{{12}{}{{}}{{}}}
\bibcite{Dhillon2}{{13}{}{{}}{{}}}
\bibcite{Dhillon}{{14}{}{{}}{{}}}
\bibcite{Filippone}{{15}{}{{}}{{}}}
\bibcite{Hartigan}{{16}{}{{}}{{}}}
\bibcite{Telgarsky}{{17}{}{{}}{{}}}
\bibcite{Slonin}{{18}{}{{}}{{}}}
\bibcite{Aronszajn}{{19}{}{{}}{{}}}
\bibcite{Gretton2012}{{20}{}{{}}{{}}}
\bibcite{Berg1984}{{21}{}{{}}{{}}}
\bibcite{Malik}{{22}{}{{}}{{}}}
\bibcite{NgJordan}{{23}{}{{}}{{}}}
\bibcite{Kernighan}{{24}{}{{}}{{}}}
\bibcite{Chan}{{25}{}{{}}{{}}}
\bibcite{Yu}{{26}{}{{}}{{}}}
\bibcite{scikit-learn}{{27}{}{{}}{{}}}
\bibcite{Vassilvitskii}{{28}{}{{}}{{}}}
\bibcite{Sapiro}{{29}{}{{}}{{}}}
\bibstyle{unsrt}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\newlabel{LastBibItem}{{29}{35}{}{}{}}
\newlabel{LastPage}{{}{35}{}{}{}}
