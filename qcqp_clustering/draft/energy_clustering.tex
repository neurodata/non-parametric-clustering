%\documentclass[aps,preprint,nofootinbib,floatfix]{revtex4-1}
\documentclass{article}

%\usepackage{nips_2017}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{nips_2017}

\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath,amssymb,amsfonts,amsthm,amscd,bm,bbm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[inline]{enumitem}
\usepackage[pdftex]{graphicx}
\graphicspath{{./figs/}}

%\usepackage{cite}
\usepackage{bbm}
\usepackage{algorithm}
\usepackage{algorithmic}

\graphicspath{{./figs}}

\hyphenation{op-tical net-works semi-conduc-tor}

\newtheorem{theorem}{Theorem}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}{Example}

%% our definitions %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\DeclareMathOperator{\aff}{aff}
\DeclareMathOperator{\st}{s.t.}
\DeclareMathOperator{\LC}{LC}
\DeclareMathOperator{\affnot}{aff_0}
\DeclareMathOperator{\conv}{conv}
\DeclareMathOperator{\relint}{relint}
\DeclareMathOperator{\vol}{vol}
\DeclareMathOperator{\range}{range}
\DeclareMathOperator{\image}{im}
\DeclareMathOperator{\nullspace}{null}
\DeclareMathOperator{\area}{area}
\DeclareMathOperator{\vspan}{span}
\DeclareMathOperator{\id}{Id}
\DeclareMathOperator{\cond}{cond}
\DeclareMathOperator{\prox}{prox}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\minimize}{minimize}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\Tr}{Tr}

\newcommand\Energy{\mathcal{E}}
\newcommand\E{\mathbb{E}}
\newcommand\kk{K}
\newcommand\kkk{h}
\newcommand\Hk{{\mathcal{H}}_{\kk}}
\newcommand\HH{\mathcal{H}}
\newcommand\C{{\mathcal{C}}}
\newcommand\Zt{\widetilde{Z}}



\title{Nonparametric Clustering Based on Energy Statistics}

\author{
Guilherme Fran\c ca \\
Johns Hopkins University\\
\texttt{guifranca@gmail.com} \\
%% examples of more authors
\And
Joshua T. Vogelstein \\
Johns Hopkins University\\
\texttt{jovo@jhu.edu}
}


%\author{Guilherme Fran\c ca}
%\email{guifranca@gmail.com}
%\affiliation{Johns Hopkins University, Center for Imaging Science}

%\author{Joshua Vogelstein}
%\email{jovo@jhu.edu}
%\affiliation{Johns Hopkins University, Center for Imaging Science}

\begin{document}

\maketitle

\begin{abstract}
blabla
\end{abstract}



\section{Introduction}

Mention why energy is important, main results, where it was applied, etc.
Motivate how this can be used for clustering. Mention most important
papers on this \ldots Explain main results of this paper and give a brief
outline.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Energy Statistics and RKHS}
\label{sec:background}

In this section we briefly review the main concepts from energy
statistics and its relation to reproducing kernel Hilbert spaces 
(RKHS), which form the basis of our work.
For more details we refer the reader
to \cite{Szkely2013} (and references therein) and also \cite{Sejdinovic2013}.

Consider random variables in $\mathbb{R}^D$ 
such that $X,X' \stackrel{iid}{\sim} P$ and 
$Y,Y' \stackrel{iid}{\sim} Q$, where $P$ and $Q$ are cumulative
distribution functions with finite first moments. 
The quantity \cite{Szkely2013}
\begin{equation}\label{eq:energy}
\Energy(P, Q) = 2 \E \| X - Y\| - \| X - X' \| - \| Y - Y' \|,
\end{equation}
called \emph{energy distance}, 
is rotational invariant and nonnegative, $\Energy(P,Q) \ge 0$, where
equality
to zero holds if and only if $P = Q$.
Above, $\| \cdot \|$ denotes the
Euclidean norm in $\mathbb{R}^D$. 
Energy distance
provides a characterization of equality of distributions, and
$\Energy^{1/2}$ is
a metric on the space of distributions.

The energy distance can be generalized as, for instance,
\begin{equation}\label{eq:energy2}
\Energy_\alpha(P, Q) = 2 \E \| X - Y\|^{\alpha} - \| X - X' \|^{\alpha} - 
\| Y - Y' \|^{\alpha},
\end{equation}
where $0<\alpha\le 2$. This quantity is also nonnegative,
$\Energy_\alpha(P,Q) \ge 0$. Furthermore, for $0<\alpha<2$ we have that
$\Energy_\alpha(P,Q) = 0$ if and only if $P=Q$, while for $\alpha=2$ 
we have $\Energy_2(P,Q) = 2\| \E(X) - \E(Y) \|^2$, showing that
equality to zero only requires
equality of the means, and thus it does not imply equality of distributions.

It is important to 
mention that \eqref{eq:energy2} can be even further generalized.
Let $X, Y \in \mathcal{X}$ and replace the Euclidean norm by
$\rho: \mathcal{X}\times\mathcal{X} \to \mathbb{R}^+$, i.e.
$\| X - Y\| \to \rho(X,Y)$, where $\rho$ is a so-called semimetric
of negative type \cite{Sejdinovic2013}, which satisfy
\begin{equation}
\label{eq:negative_type}
\sum_{i,j=1}^n \alpha_i \alpha_j \rho(X_i, X_j) \le 0,
\end{equation}
where $X_i \in \mathcal{X}$, and $\alpha_i \in \mathbb{R}$ such that
$\sum_{i=1}^n \alpha_i = 0$.
In this case, there is a Hilbert space $\mathcal{H}$ and
a map $\varphi: \mathcal{X} \to
\mathcal{H}$ such that
$\rho(X, Y) = \| \varphi(X) - \varphi(Y) \|_{\mathcal{H}}^2$. 
Even though the semimetric 
$\rho$ may not satisfy the triangle inequality, 
$\rho^{1/2}$ does since it can be shown to be a legit metric. 

There is an equivalence between energy distance, 
commonly used in statistics,
and distances between embeddings of distributions in 
RKHS, commonly used in machine learning. 
This equivalence was established
in \cite{Sejdinovic2013}. Let us first recall the definition of
RKHS. Let $\HH$ be a Hilbert space of real-valued functions
over $\mathcal{X}$. A function 
$\kk : \mathcal{X} \times \mathcal{X} \to 
\mathbb{R}$ is a reproducing kernel of $\HH$ if it satisfies
the following two conditions:
\begin{enumerate}
\item $\kkk_x \equiv \kk(\cdot, x) \in \HH$ 
for any $x \in \mathcal{X}$.
\item $\langle \kkk_x, f \rangle_{\HH} = f(x)$ for
any $x\in\mathcal{X}$ and $f\in \HH$.
\end{enumerate}
In other words, for any $x \in \mathcal{X}$ there is a unique function
$\kkk_x \in \HH$ that reproduces $f(x)$ through the inner product
of $\HH$.
If such a \emph{kernel} 
function $\kk$ exists, then $\HH$ is called a RKHS.
From this we have $\langle \kkk_x, \kkk_y \rangle = \kkk_y(x) = \kk(x,y)$. 
This implies
that $\kk(x,y)$ is symmetric and positive definite, 
$\sum_{i,j=1}^n c_i c_j
\kk(x_i,x_j) \ge 0$ for $c_i,c_j \in \mathbb{R}$.

The Moore-Aronszajn theorem establishes the converse \cite{Aronszajn}.
For every symmetric
and positive definite function $\kk: \mathcal{X}\times \mathcal{X} \to
\mathbb{R}$, there is an associated RKHS, $\Hk$, 
with reproducing
kernel $\kk$. The map $\varphi: x \mapsto \kkk_x \in \Hk$ is called
the canonical feature map. Given a kernel $\kk$,
this theorem enables us to define an embedding of a probability measure
$P$ into the RKHS: $P \mapsto \kkk_P \in
\Hk$ such that 
$\int f(x) d P(x) = \langle f, \kkk_P \rangle$ for all $f \in \Hk$,
or alternatively $\kkk_P = \int \kk(\cdot, x)  d P(x)$. 
We can now  introduce the 
notion of distance between two probability measures using the inner product
of $\Hk$. This is called the maximum mean discrepancy (MMD) and
is given by
\begin{equation}\label{eq:mmd}
\gamma_\kk(P,Q) = \| \kkk_P - \kkk_Q \|_{\Hk},
\end{equation}
which can also be written as \cite{Gretton2012}
\begin{equation}\label{eq:mmd2}
\gamma_\kk^2(P,Q) = \E \kk(X,X') + \E \kk(Y,Y') - 2 \E \kk(X, Y)
\end{equation}
where $X,X' \stackrel{iid}{\sim} P$ and $Y,Y'\stackrel{iid}{\sim} Q$.
From the equality between \eqref{eq:mmd} and \eqref{eq:mmd2} we also
have 
\begin{equation}\label{eq:inner_data}
\langle \kkk_P, \kkk_Q \rangle_{\Hk} = \E \, \kk(X, Y).
\end{equation}
Therefore, in practice, we can estimate the inner product between the 
embedded distributions 
by averaging the kernel function over sampled data.

The following important result shows that semimetrics of negative
type and symmetric positive definite kernels are closely related
\cite{Berg1984}. Let $\rho: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$
be a semimetric, 
and $x_0 \in \mathcal{X}$ an arbitrary but fixed point.
Define
\begin{equation}
\label{eq:kernel_semimetric}
\kk(x,y) = \tfrac{1}{2} \left\{  \rho(x,x_0) + \rho(y,x_0) - \rho(x,y)\right\}.
\end{equation}
Then, $\kk$ is positive definite if and only if $\rho$ is of negative type
\eqref{eq:negative_type}.
Here we have a family of kernels, one for each choice of $x_0$. Conversely,
if $\rho$ is a semimetric of negative type and $\kk$ is a kernel in this
family, then 
\begin{equation}
\label{eq:gen_kernel}
\rho(x,y) = \kk(x,x) + \kk(y,y) -2\kk(x,y) = \| \kkk_x - \kkk_y
\|^2_{\Hk},
\end{equation}
and the canonical feature map 
$\varphi: x \mapsto \kkk_x$ is injective \cite{Sejdinovic2013}.
We say that the kernel $\kk$ generates the semimetric $\rho$. 
If two different kernels generate the same $\rho$, they are
equivalent kernels.

Now we can state the equivalence between energy distance $\Energy$ and
inner products on RKHS, which is one of the main results of
\cite{Sejdinovic2013}. If $\rho$ is a semimetric
of negative type and $\kk$ a kernel that generates $\rho$, then
\begin{align}
\Energy(P, Q) &\equiv 2\E \, \rho(X,Y) - \E \, \rho(X,X') - \E \, \rho(Y,Y') 
\label{eq:EphoDef}\\
&= 2 \left[ \E \, \kk(X, X') + \E \, \kk(Y, Y') - 2\E \, \kk(X, Y)\right] \\
&=2 \gamma_\kk^2(P,Q) 
\end{align}
This result follows simply by substituting \eqref{eq:gen_kernel} into
\eqref{eq:EphoDef}, and using \eqref{eq:mmd2}.
Since $\gamma_k^2(P, Q) = \| \kkk_P - \kkk_Q \|^2_{\Hk}$, we
can compute the energy distance using the inner product of $\Hk$. Moreover,
this can be computed from the data according to \eqref{eq:inner_data}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Energy Distance based Clustering}
\label{sec:clustering_theory}

Now we formulate an optimization problem for clustering based on 
energy statistics
and RKHS, introduced in the previous section.
Assume we have data $\mathbb{X} = \{ x_1,x_2,\dotsc, x_n \}$, where
$x_i \in \mathcal{X}$ is in some space of 
negative type \eqref{eq:negative_type},
and a partition $\mathbb{X} = \cup_{j=1}^k \C_j$, where
$\C_i \cap \C_j = \emptyset$.
Each expectation in 
\eqref{eq:EphoDef}
can be computed 
through the function
\begin{equation}
\label{eq:g_def}
g (\C_i, \C_j) \equiv 
\dfrac{1}{n_i n_j}
\sum_{x \in \C_i} 
\sum_{y \in \C_j} \rho(x, y)
\end{equation}
where $n_i = |\C_i|$ is the number of elements in partition
$\C_i$. In energy statistics 
\cite{Szkely2013} we have the within energy dispersion
\begin{equation}
\label{eq:within}
W = 
\sum_{j=1}^{k} \dfrac{n_j}{2} g(\C_j, \C_j),
\end{equation}
and also the between-sample energy statistic
\begin{equation}
S = 
\sum_{1 \le  j < l \le k } \dfrac{n_j n_{l}}{2 n} \left[
2 g(\C_j, \C_l) - 
g(\C_j, \C_j) - 
g(\C_l, \C_l)
\right],
\end{equation}
where $n=\sum_{j=1}^{k} n_j$. Given a set of distributions
$\{ P_j\}_{j=1}^k$ where $x \in \C_j \sim P_j$, the quantity
$S_\alpha$ provides
a \emph{nonparametric} test statistic for equality of distributions.
Under the null hypothesis $H_0: P_1=P_2=\dotsm=P_k$, $S_\alpha$ is small, 
and under
the alternative hypothesis $H_1: P_i \ne P_j$ for at least two $i\ne j$, 
$S_\alpha \to \infty$ as the sample size is large, $n \to \infty$.
This test is nonparametric in the sense that it does not make any assumptions
about the distributions $P_j$.

Based on this test statistic for equality of distributions, a possible
criteria for clustering data is to 
maximize $S$. However, 
it can be shown that the total dispersion of the data obeys \cite{Szkely2013}
\begin{equation}
T(\mathbb{X}) 
= W + S = \dfrac{n}{2}
g(\mathbb{X}, \mathbb{X}). 
\end{equation}
Note that $T$ only depends on the pooled data, so it 
does not depend on how we partition $\mathbb{X}$. Therefore, maximizing
$S$ is equivalent to minimizing $W$, which
has a simpler form. Thus, our clustering problem corresponds to
find the best partition of $\mathbb{X}$ to solve the optimization problem
\begin{equation}
\label{eq:minimize}
\min_{\{ \C_j \} } W(\{ 
\C_1, \dotsc, \C_k
\})
\end{equation}
where each datapoint belongs to one and only one partition (hard assignments).

Now fix an arbitrary point $x_0 \in \mathcal{X}$, and 
suppose that the kernel $K$ generates $\rho$, such that 
\eqref{eq:kernel_semimetric} and \eqref{eq:gen_kernel} hold.
Then we can write \eqref{eq:within} as
\begin{equation}\label{eq:W2}
W(\{ \C_j \} )
= \dfrac{1}{2} \sum_{j=1}^k \dfrac{1}{n_j} \sum_{x,y \in \C_j} \rho(x,y)
= \sum_{j=1}^k \sum_{x \in \C_j}  \bigg(
\kk(x,x) - \dfrac{1}{n_j} \sum_{y \in \C_j} \kk(x,y) \bigg)
\end{equation}
When minimizing $W$, the first term is just a constant and does
not contribute,
therefore the problem reduces to
\begin{equation}
\label{eq:max_prob}
\max_{ \{ \C_j \} } \sum_{j=1}^k \dfrac{1}{n_j} \sum_{x,y\in C_j} \kk(x,y) .
\end{equation}
Let us introduce the binary matrix $Z \in \{ 0,1 \}^{n\times k}$ 
such that 
\begin{equation}
Z_{ij} = \begin{cases}
1 & \mbox{if $x_i \in \C_j$ } \\
0 & \mbox{otherwise.}
\end{cases}
\end{equation}
Notice that $D = Z^T Z = \diag( n_1, n_2, \dotsc, n_k )$ contains the number
of elements in each partition. Introducing the kernel matrix
$G \in \mathbb{R}^{n\times n}$ such that
\begin{equation}
\label{eq:kernel_matrix}
G_{ij} = \kk(x_i, x_j),
\end{equation}
then \eqref{eq:max_prob} is equal to
$\max \Tr \left\{ D^{-1} Z^\top G Z \right\}$. 
Therefore, our clustering optimization problem based on energy statistics
can be formulated as
\begin{equation}\label{eq:qcqp}
\begin{aligned}
& \max_{Z} \Tr\left\{ \big( Z D^{-1/2}\big)^\top G 
\big( ZD^{-1/2} \big) 
\right\} \\
&\mbox{s.t. $Z_{ij} \in \{0,1\}$, $\sum_{j=1}^k Z_{ij} = 1$, 
$\sum_{i=1}^n Z_{ij} = n_j$, and $D = Z^\top Z$}.
\end{aligned}
\end{equation}
This is a quadratic problem with integer constraints, which is 
NP-hard. Let us now write this in terms of 
$\Zt \equiv Z D^{-1/2}$, which componentwise is 
\begin{equation}
\Zt_{ij} = \begin{cases}
\tfrac{1}{\sqrt{n_j}} & \mbox{if $x_i \in \C_j$ } \\
0 & \mbox{otherwise.}
\end{cases}
\end{equation}
We thus have
\begin{equation}
\label{eq:qcqp2}
\max_{\Zt} \Tr \left\{ \Zt^\top G \Zt \right\}  \qquad
\mbox{s.t. $\Zt \ge 0$, $\Zt^\top Z = I$, 
$\Zt \Zt^\top \bm{e} = \bm{e}$},
\end{equation}
where $\bm{e} = (1,1,\dots,1) \in \mathbb{R}^n$ is the all-ones vector,
and $G$ is the pairwise kernel matrix \eqref{eq:kernel_matrix} obtained
from \eqref{eq:kernel_semimetric}.
Therefore, to cluster data $\{ x_i \}_{i=1}^n \in \mathcal{X}$ into 
$k$ partitions,
assuming that $k$ is given, we first compute $G$  --- 
which is  defined by an arbitrary semimetric of negative 
type on $\mathcal{X}$  --- and then 
solve the optimization problem \eqref{eq:qcqp2} for $\Zt \in
\mathbb{R}^{n\times k}$. The $i$th row
of $\Zt$ will contain a single nonzero element in some $j$th column,
indicating that $x_i \in \C_j$.

In general, 
problem \eqref{eq:qcqp2} is NP-hard since it is a quadratically
constrained quadratic problem (QCQP). There are  
few methods available to tackle this kind of problem directly,
which is computational prohibitive even for relatively small datasets.
However, one can find an approximate solution by relaxing some 
of the constraints. For instance, an approximation can be given
by 
$\max_{Y} \Tr\left\{ Y^\top G \, Y \right\}$ subject to $Y^\top Y = I$,
and requiring that the rows of $Y$ are normalized. 
It is possible to find a global solution to this problem by
choosing $Y$ as the top $k$ eigenvectors of $G$, which results
in 
$\max \Tr \left\{ Y^\top G \, Y \right\}  = \sum_{i=1}^k \lambda_i(G)$, 
which is the
sum of the top $k$ eigenvalues of $G$.

It is important to note that \eqref{eq:qcqp2} has the same
formulation as kernel $k$-means, spectral clustering, and the maximum
cut problem on graphs \cite{Dhillon}. The 
result \eqref{eq:qcqp2} brings energy statistics based clustering
into this broad picture, and \eqref{eq:qcqp2} should have 
interesting applications in
graph partitioning problems and unsupervised learning in general. 
Furthermore, and most importantly, our analysis is valid
for any space $\mathcal{X}$ equiped with a semimetric of negative type.
This method is
nonparametric since it does not assume any form of the 
distribution of the data,
contrary to $k$-means and gaussian mixture models (GMM), for example.
Also, there is no concept of cluster center involved in this approach.


\subsection{A Simple Algorithm}

We can formulate an iterative algorithm to find
an approximate solution to \eqref{eq:qcqp2} on the same lines
as kernel $k$-means.
Let $t$ be the iteration time. First, precompute the kernel
matrix $G$, fix the number of clusters $k$, then
perform the following steps:
\begin{enumerate}
\item \label{initialize} 
Initialize clusters $\{ \C_1^{(0)},\dotsc,\C_k^{(0)} \}$, which
determines the label matrix $\Zt^{(0)}$.
\item \label{assignment} For each datapoint $x_i$ compute
its cluster assignment through
\begin{equation}
\label{eq:algo}
\Zt^{(t+1)}_{ij} = 
\begin{cases}
\tfrac{1}{\sqrt{n_j}} & \mbox{if $j = \argmax_\ell \tfrac{1}{n_\ell} 
\sum_{m=1}^n G_{i m} \Zt^{(t)}_{m\ell}$ } \\
0 & \mbox{otherwise.}
\end{cases}
\end{equation}
\item If converged return $\Zt^{(t+1)}$, otherwise
set $t = t+1$ and repeat step~\ref{assignment}.
\end{enumerate}
If data is $D$-dimensional, computing $G$ has complexity
$O(n^2 D)$. Step \ref{assignment} above has complexity $O(n)$ for
each point, thus total complexity $O(n^2)$. Assuming we perform $T$ 
iterations,
the total complexity of the algorithm is $O(n^2(D + T))$.
We can initialize the algorithm in step~\ref{initialize} with any method
we want, a good alternative is the initialization from $k$-means++.

\subsection{Two-Class Problem in One Dimension}

If data is one-dimensional and we choose
$\rho(x,y) = |x - y|$, we can actually compute 
\eqref{eq:g_def} in $O(n \log n)$ instead of $O(n^2 D)$ and find
a direct solution to \eqref{eq:minimize}. 
This is done by noticing that
\begin{equation}
|x - y|  = \mathbbm{1}(x \ge y ) (x - y) -
\mathbbm{1}(x < y) (x - y).
\end{equation}
Denote
\begin{equation}
n_j^{-} (x) = \sum_{y \in \C_j} \mathbbm{1}(y \le x), \quad  
n_j^{+} (x) = \sum_{y \in C_j} \mathbbm{1}(y > x), \quad
\mathcal{D}_j(x) = \dfrac{n_j^-(x) - n_j^+(x)}{n_j}
\end{equation}
where $n_j^+(x)$ is the number of elements in $\C_j$ which are larger
than $x \in \C_i$, and so on.
Now \eqref{eq:g_def} can be written as
\begin{equation}
\label{eq:g1d}
g(\C_i, \C_j) = 
\dfrac{1}{n_i} \sum_{x \in \C_i} \mathcal{D}_j(x) \, x - 
\dfrac{1}{n_j} \sum_{y \in \C_j} \mathcal{D}_i(y) \, y.
\end{equation}
If we merge $\C_i$ and $\C_j$ and sort the data, this expression 
can be computed in $O(n)$, where $n=n_i+n_j$.
Now for a two-class problem, we have to loop through each point
and compute \eqref{eq:within} to find the best split. Since sorting
sorting usually takes $O(n \log n)$, the complexity of
this procedure is $O(n(\log n + n)) = O(n^2)$.

\section{Numerical Experiments}

\section{Conclusion}


\subsection*{Acknowledgements}
We thank \ldots


\bibliographystyle{unsrt}
\bibliography{biblio.bib}



\end{document}
